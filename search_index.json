[["index.html", "Introduction to Statistical Learning Using R Book Club Welcome", " Introduction to Statistical Learning Using R Book Club The R4DS Online Learning Community 2021-12-13 Welcome This is a companion for the book Introduction to Statistical Learning Using R by Gareth James, Daniela Witten, Trevor Hastie, and Rob Tibshirani (Springer Science+Business Media, LLC, part of Springer Nature, copyright 2021, 978-1-0716-1418-1_1). This companion is available at r4ds.io/islr. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["st-edition-vs-2nd-edition.html", "1st edition vs 2nd edition", " 1st edition vs 2nd edition This club is reading the digital version of the second edition of this book (2e). "],["pace.html", "Pace", " Pace This book is often used for two-semester-long courses. We’ll try to cover 1 chapter/week, but… …It’s ok to split chapters when they feel like too much. We will try to meet every week, but will likely take some breaks for holidays, etc. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Learning objectives: Recognize various types of statistical learning. Understand why this book is useful for you. Be able to read mathematical notation used throughout this book. Describe the overall layout of this book. Be able to find data used in examples throughout the book. "],["what-is-statistical-learning.html", "1.1 What is statistical learning?", " 1.1 What is statistical learning? More about this in Chapter 2. Supervised: “Building a model to predict an output from inputs.” Predict wage from age, education, and year. Predict market direction from previous days' performance. Unsupervised: Inputs but no specific outputs, find relationships and structure. Identify clusters within cancer cell lines. "],["why-islr.html", "1.2 Why ISLR?", " 1.2 Why ISLR? “Facilitate the transition of statistical learning from an academic to a mainstream field.” Machine learning* is useful to everyone, let’s all learn enough to use it responsibly. R “labs” make this make sense for this community! "],["notation.html", "1.3 Notation", " 1.3 Notation n = number of observations (rows) p = number of features/variables (columns) We’ll come back here if we need to as we go! Some symbols they assume we know: \\(\\in\\) = “is an element of”, “in” \\({\\rm I\\!R}\\) = “real numbers” "],["what-have-we-gotten-ourselves-into.html", "1.4 What have we gotten ourselves into?", " 1.4 What have we gotten ourselves into? 2: Terminology &amp; main concepts 3-4: Classic linear methods 5: Resampling (so we can choose the best method) 6: Modern updates to linear methods 7+: Beyond Linearity (we can worry about details as we get there) "],["wheres-the-data.html", "1.5 Where’s the data?", " 1.5 Where’s the data? install.packages(&quot;ISLR2&quot;) Or “install” this book. install.packages(&quot;remotes&quot;) remotes::install_github(&quot;r4ds/bookclub-islr&quot;) remove.packages(&quot;bookclubislr&quot;) # This isn&#39;t really a package. "],["meeting-videos.html", "1.6 Meeting Videos", " 1.6 Meeting Videos 1.6.1 Cohort 1 Meeting chat log ADD LOG HERE "],["learning.html", "Chapter 2 Statistical Learning", " Chapter 2 Statistical Learning Learning objectives: Understand Vocabulary for prediction Understand “Error”/Accuracy Understand Parametric vs Nonparametric Models Describe the trade-off between more accurate models and more interpretable models. Compare and contrast supervised and unsupervised learning. Compare and contrast regression and classification problems. Measure the accuracy/goodness of regression model fits. Measure the accuracy/goodness of classification model fits. Describe how bias and variance contribute to the model error. Understand overfitting. Recognize KNN. Understand the role of tuning in ML models. "],["what-is-statistical-learning-1.html", "2.1 What is Statistical Learning?", " 2.1 What is Statistical Learning? The chapter opens with a discussion of what models are good for, how we use them, and some of the issues that are involved. Can we predict Sales using ad media spending? read_csv( &quot;https://www.statlearning.com/s/Advertising.csv&quot;, col_types = &quot;-dddd&quot;, skip = 1, col_names = c(&quot;row&quot;, &quot;TV&quot;, &quot;radio&quot;, &quot;newspaper&quot;, &quot;sales&quot;) ) %&gt;% pivot_longer(cols = -sales) %&gt;% ggplot(aes(value, sales)) + geom_point(shape = 21, color = &quot;red&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;, se = FALSE, formula = &quot;y ~ x&quot;) + facet_wrap(vars(name), scales = &quot;free_x&quot;, strip.position = &quot;bottom&quot;) + theme_bw() + theme(strip.placement = &quot;outside&quot;, panel.grid = element_blank(), strip.background = element_blank()) + labs(x = NULL, caption = &quot;Source: https://www.statlearning.com/s/Advertising.csv | A simple least squares fit shown in blue&quot;) You can make out a trend, at least in radio and TV. We typically want to be able to characterize the Sales potential as a function of all three media inputs. How do they operate together? Notation In this setting, the advertising budgets are input variables while sales is an output variable. The input variables are typically denoted using the symbol \\(X\\), with a subscript to distinguish them. So \\(X_1\\) might be the TV budget, \\(X_2\\) the radio budget, and \\(X_3\\) the newspaper budget. The inputs go by different names, such as predictors, independent variables, features, or sometimes just variables. The output variable in this case, sales is often called the response or dependent variable, and is typically denoted using the symbol \\(Y\\). We assume that there is some relationship between \\(Y\\) and \\(X = (X_1, X_2,\\dots,X_p)\\), which can be written in the very general form \\[\\begin{equation} y = f(X) + \\epsilon \\end{equation}\\] Here \\(f\\) is some fixed but unknown function of \\(X_1, \\dots, X_p\\), and \\(\\epsilon\\) is a random error term, which is independent of \\(X\\) and has a mean of zero. In this formulation, \\(f\\) represents the systematic information that \\(X\\) provides about \\(Y\\). What is the \\(f(X)\\) good for? With a good \\(f\\) we can make predictions of \\(Y\\) at new points. We can understand which components are important in explaining \\(Y\\), and which components are irrelevant. Depending on the complexity of \\(f\\), we may be able to understand how each component of \\(X_j\\) of \\(X\\) affects \\(Y\\). read_csv(&quot;https://www.statlearning.com/s/Income1.csv&quot;, col_types = &quot;-dd&quot;, skip = 1, col_names = c(&quot;row&quot;, &quot;Education&quot;,&quot;Income&quot;)) %&gt;% mutate(res = residuals(loess(Income ~ Education))) %&gt;% ggplot(aes(Education, Income)) + geom_point(shape = 20, size = 4, color = &quot;red&quot;) + geom_segment(aes(xend = Education, yend = Income - res)) + geom_smooth(method = &quot;loess&quot;, se = FALSE, color = &quot;blue&quot;, formula = &quot;y ~ x&quot;) + scale_x_continuous(breaks = seq(10,22,2)) + theme_bw() + theme(panel.grid = element_blank()) + labs(x = &quot;Years of Education&quot;, caption = &quot;Source: https://www.statlearning.com/s/Income1.csv | A loess fit shown in blue&quot;) The vertical lines represent the error terms \\(\\epsilon\\). That is, the difference between the model estimate and the observed value. 2.1.1 Why Estimate \\(f\\)? There are two main reasons that we may wish to estimate \\(f\\): prediction and inference. Prediction In many situations, a set of inputs \\(X\\) are readily available, but the output \\(Y\\) cannot be easily obtained. We can predict \\(Y\\) using \\[\\begin{equation} \\hat{Y} = \\hat{f}(X) \\end{equation}\\] \\(\\hat{f}\\) is often treated as a black box, One is not typically concerned with the exact form of \\(\\hat{f}\\), provided that it yields accurate predictions for Y. The accuracy of \\(\\hat{Y}\\) as a prediction for \\(Y\\) depends on two quantities, called the reducible error and the irreducible error. Reducible Error We can reduce the gap between our estimate and the true function by applying improved methods. Irreducible Error The quantity \\(\\epsilon\\) may contain unmeasured variables that are useful in predicting \\(Y\\) : since we don’t measure them, \\(f\\) cannot use them for its prediction. The quantity \\(\\epsilon\\) will also contain unmeasurable variation. The focus of the book is on techniques for estimating \\(f\\) with the aim of minimizing the reducible error. Inference We are often interested in understanding the association between \\(Y\\) and \\(X = (X_1, X_2,\\dots,X_p)\\). In this situation we wish to estimate \\(f\\), but our goal is not necessarily to make predictions for \\(Y\\). Now \\(f\\) cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions: Which predictors are associated with the response? What is the relationship between the response and each predictor? Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated? Consider the Advertising data. One may be interested in answering questions such as: – Which media are associated with sales? – Which media generate the biggest boost in sales? or – How large of an increase in sales is associated with a given increase in TV advertising? 2.1.2 How do we estimate \\(f\\)? We want to find a function \\(f\\) such that \\(Y ≈ f(X)\\) for any observation. Broadly speaking, most statistical learning methods for this task can be characterized as either parametric or non-parametric. Parametric Methods Parametric methods involve a two-step model-based approach. First we make an assumption about the functional form of \\(f\\) After a model is selected, we apply a procedure to fit or train the model. The potential disadvantage of a parametric approach is that the model we choose will usually not match the true unknown form of \\(f\\). If the chosen model is too far from the true \\(f\\), then our estimate will be poor. We can try to address this problem by choosing more flexible models that can fit many different possible functional forms for \\(f\\). These more complex models can lead to a phenomenon known as overfitting the data, which essentially means they follow the errors, or noise, too closely. ISLR2::Portfolio %&gt;% ggplot(aes(X, Y)) + geom_point(shape = 21, color = &quot;gray50&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE, formula = &quot;y ~ x&quot;) + geom_point(x = 1, y = 0.5, shape = 20, color = &quot;red&quot;, size = 8) + geom_vline(xintercept = 1, color = &quot;red&quot;) + theme_bw() + theme(panel.grid = element_blank()) + labs(caption = &quot;Source: ISLR2::Portfolio | A linear fit shown in red&quot;) In a given dataset, there may be many observed values of \\(Y\\) for each \\(X\\). In this regression example, the expected value of Y given X = 1 is 0.5. The value of 0.5 is an average of sorts of the slice taken at x = 1. Although it is almost never correct, a linear model often serves as a good and interpretable approximation. Non-Parametric Methods Non-parametric methods don’t make explicit assumptions about the functional form of \\(f\\) Instead they seek an estimate of \\(f\\) that gets as close to the data points as possible without being too rough or wiggly. Any parametric approach brings with it the possibility that the functional form used to estimate \\(f\\) on training data is very different from the true \\(f\\), in which case the resulting model will not fit unseen, new data well. Non-parametric approaches do suffer from a major disadvantage: A very large number of observations (far more a than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for \\(f\\). 2.1.3 Prediction Accuracy vs Model Interpretability Why would we ever choose to use a more restrictive method instead of a very flexible approach? If we are mainly interested in inference, then restrictive models are often more interpretable. Flexible approaches can lead to such complicated estimates of \\(f\\) that it is difficult to understand how any individual predictor is associated with the response. 2.1.4 Supervised Versus Unsupervised Learning Supervised problems have labeled data with a response measurement, also called a dependent variable. Unsupervised problems have no labels. The challenge is to derive clusters, or patterns, to better understand what is happening. 2.1.5 Regression Versus Classification Problems Variables can be characterized as either quantitative or qualitative (categorical). Quantitative variables take on numerical values. In contrast, qualitative variables take on values in classes, or categories. "],["assessing-model-accuracy.html", "2.2 Assessing Model Accuracy", " 2.2 Assessing Model Accuracy There is no free lunch in statistics: no one method dominates all others over all possible problems. Selecting the best approach can be challenging in practice. 2.2.1 Measuring Quality of Fit There is no free lunch in statistics: no one method dominates all others over all possible data sets. On a particular data set, one specific method may work best, but some other method may work better on a similar but different data set. MSE We always need some way to measure how well a model’s predictions actually match the observed data. In the regression setting, the most commonly-used measure is the mean squared error (MSE), given by \\[ MSE = \\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{f}(x_i))^2,\\] The MSE will be small if the predicted responses are very close to the true responses,and will be large if for some of the observations, the predicted and true responses differ substantially. Training vs. Test The MSE in the above equation is computed using the training data that was used to fit the model, and so should more accurately be referred to as the training MSE. In general, we do not really care how well the method works on the training data. We are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data. \\[\\mathrm{Ave}(y_0 - \\hat{f}(x_0))^2 ,\\] We’d like to select the model for which this quantity is as small as possible on unseen, future samples. The degrees of freedom is a quantity that summarizes the flexibility of a curve. The training MSE declines monotonically as flexibility increases. Overfitting As model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE. MSE, for a given value, can always be decomposed into the sum of three fundamental quantities: the variance of \\(\\hat{f}(x_0)\\), the squared bias of \\(\\hat{f}(x_0)\\) and the variance of the error terms \\(\\epsilon\\). That is, \\[E\\big(y_0 - \\hat{f}(x_0)\\big)^2 = \\mathrm{Var}\\big(\\hat{f}(x_0)\\big) +[\\mathrm{Bias}\\big(\\hat{f}(x_0)\\big)]^2 + \\mathrm{Var}(\\epsilon)\\] Here the notation \\(E\\big(y_0 - \\hat{f}(x_0)\\big)^2\\) defines the expected test MSE at \\(x_0\\) and refers to the average test MSE that we would obtain if we repeatedly estimated \\(f\\) using a large number of training sets, and tested each at \\(x_0\\). The overall expected test MSE can be computed by averaging \\(E \\big(y_0 - \\hat{f}x(x_0)\\big)^2\\) over all possible values of \\(x_0\\) in the test set FIGURE 2.9. Left: Data simulated from f, shown in black. Three estimates of f are shown: the linear regression line (orange curve), and two smoothing spline fits (blue and green curves). Right: Training MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed line). Squares represent the training and test MSEs for the three fits shown in the left-hand panel. The orange, blue and green squares indicate the MSEs associated with the corresponding curves in the left hand panel. A more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve—note that in Figure 2.9, linear regression is at the most restrictive end, with two degrees of freedom. The training MSE declines monotonically as flexibility increases. As the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a U-shape in the test MSE. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. As model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. FIGURE 2.10. Details are as in Figure 2.9, using a different true f that is much closer to linear. In this setting, linear regression provides a very good fit to the data. Another example in which the true \\(f\\) is approximately linear. However, because the truth is close to linear, the test MSE only decreases slightly before increasing again, so that the orange least squares fit is substantially better than the highly flexible green curve. Figure 2.11 displays an example in which f is highly non-linear. The training and test MSE curves still exhibit the same general patterns, but now there is a rapid decrease in both curves before the test MSE starts to increase slowly. FIGURE 2.11. Details are as in Figure 2.9, using a different f that is far from linear. In this setting, linear regression provides a very poor fit to the data. We need to select a statistical learning method that simultaneously achieves low variance and low bias. 2.2.2 The Bias-Variance Trade-Off As we use more flexible methods, the variance will increase and the bias will decrease. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases. FIGURE 2.12. Squared bias (blue curve), variance (orange curve), Var(ϵ) (dashed line), and test MSE (red curve) for the three data sets in Figures 2.9–2.11. The vertical dotted line indicates the flexibility level corresponding to the smallest test MSE. In all three cases, the variance increases and the bias decreases as the method’s flexibility increases. The relationship between bias, variance, and test set MSE given is referred to as the bias-variance trade-off. The challenge lies in finding a method for which both the variance and the squared bias are low. This trade-off is one of the most important recurring themes in this book. 2.2.3 The Classification Setting The most common approach for quantifying the accuracy of our estimate \\(\\hat{f}\\) is the training error rate, the proportion of mistakes that are made if we apply our estimate \\(\\hat{f}\\) to the training observations: \\[\\frac{1}{n}\\sum_{i=1}^{n}I(y_i \\ne \\hat{y}_i).\\] The above equation computes the fraction of incorrect classifications. The equation is referred to as the training error rate because it is computed based on the data that was used to train our classifier. Again, we are most interested in the error rates that result from applying our classifier to test observations that were not used in training. Test Error The test error rate associated with a set of test observations of the form \\((x_0, y_0)\\) is given by \\[\\mathrm{Ave}\\big(I(y_i \\ne \\hat{y}_i)\\big).\\] Where \\(\\hat{y}_0\\) is the predicted class label that results from applying the classifier to the test observation with predictor \\(x_0\\). A good classifier is one for which the test error is smallest. The Bayes Classifier Hypothetical – cannot be done in practice The test error rate is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values. In other words, we should simply assign a test observation with predictor vector \\(x_0\\) to the class \\(j\\) for which \\[\\mathrm{Pr}(Y=j|X=x_0).\\] Note that is a conditional probability: it is the probability that \\(Y = j\\), given the observed predictor vector \\(X_0\\). This very simple classifier is called the Bayes classifier. Bayes Classifier Decision Boundary Figure 2.13 provides an example using a simulated data set in a two dimensional space consisting of predictors X1 and X2. The orange and blue circles correspond to training observations that belong to two different classes. For each value of X1 and X2, there is a different probability of the response being orange or blue. FIGURE 2.13. A simulated data set consisting of 100 observations in each of two groups, indicated in blue and in orange. The purple dashed line represents the Bayes decision boundary. The orange background grid indicates the region in which a test observation will be assigned to the orange class, and the blue background grid indicates the region in which a test observation will be assigned to the blue class. The purple dashed line represents the points where the probability is exactly 50%. This is called the Bayes decision boundary. An observation that falls on the orange side of the boundary will be assigned to the orange class, and similarly an observation on the blue side of the boundary will be assigned to the blue class. The Bayes classifier produces the lowest possible test error rate, called the Bayes error rate. The overall Bayes error rate is given by \\[1- E\\big(\\mathop{\\mathrm{max}}_{j}(Y=j|X)\\big),\\] where the expectation averages the probability over all possible values of X. The Bayes error rate is analogous to the irreducible error, discussed earlier. K-Nearest Neighbors For real data, we do not know the conditional distribution of \\(Y\\) given \\(X\\), and so computing the Bayes classifier is impossible. Many approaches attempt to estimate the conditional distribution of \\(Y\\) given \\(X\\), and then classify a given observation to the class with highest estimated probability. One such method is the K-nearest neighbors (KNN) classifier. Given a positive integer K and a test observation \\(t_{0}\\), the KNN classifier first identifies the K points in the training data that are closest to \\(x_0\\) \\[\\mathrm{Pr}(Y=j|X=x_0) = \\frac{1}{K}\\sum_{i \\in \\mathcal{N}_0} I (y_i = j)\\] Finally, KNN classifies the test observation \\(x_0\\) to the class with the largest probability. Figure 2-143 The KNN approach with \\(K\\) = 3 at all of the possible values for \\(X_1\\) and \\(X_2\\), and have drawn in the corresponding KNN decision boundary. Despite the fact that it is a very simple approach, KNN can produce classifiers that are surprisingly close to the optimal Bayes classifier. KNN with Different K FIGURE 2.15. The black curve indicates the KNN decision boundary on the data from Figure 2.13, using K = 10. The Bayes decision boundary is shown as a purple dashed line. The KNN and Bayes decision boundaries are very similar. FIGURE 2.16. A comparison of the KNN decision boundaries obtained using K = 1 and K = 100 on the data from Figure 2.13. With K = 1, the decision boundary is overly flexible, while with K = 100 it is not sufficiently flexible. The Bayes decision boundary is shown as a purple dashed line. The choice of K has a drastic effect on the KNN classifier obtained. Nearest neighbor methods can be lousy when \\(p\\) is large. Reason: the curse of dimensionality. Nearest neighbors tend to be far away in high dimensions. KNN Tuning FIGURE 2.17. The KNN training error rate (blue, 200 observations) and test error rate (orange, 5,000 observations) on the data from Figure 2.13, as the level of flexibility (assessed using 1/K on the log scale) increases, or equivalently as the number of neighbors K decreases. The black dashed line indicates the Bayes error rate. The jumpiness of the curves is due to the small size of the training data set. As we use more flexible classification methods, the training error rate will decline but the test error rate may not. As \\(1/K\\) increases, the method becomes more flexible. As in the regression setting, the training error rate consistently declines as the flexibility increases. However, the test error exhibits a characteristic U-shape, declining at first (with a minimum at approximately \\(K\\) = 10) before increasing again when the method becomes excessively flexible and overfits. "],["exercises.html", "2.3 Exercises", " 2.3 Exercises Conceptual 1. For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer. (a) The sample size n is extremely large, and the number of predictors p is small. A flexible model can take advantage of the large number of observations to make a detailed model. (b) The number of predictors p is extremely large, and the number of observations n is small. I’d apply Principal Component Analysis to drive p well under n, and then an inflexible, parametric method like Ordinary Least Squares could provide a model with a reasonable test set performance. (c) The relationship between the predictors and response is highly non-linear. A flexible model could better capture the non-linearity. (d) The variance of the error terms, i.e. σ2 = Var(ϵ), is extremely high. An inflexible, parametric method would provide a standardized, average slice across the domain. 2. Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p. (a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary. The problem is a regression, and we are interested in inferring the component factors. N is 500, and p is 3. (b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables. The problem is a binary classification prediction. The N is 20, p is 13. (c) We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market. The % change problem is a regression prediction with a p of 4 and an N of 52. 3. We now revisit the bias-variance decomposition. (a) Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one. X-axis: increasing flexibility and model complexity Y-axis: increasing Error, Variance, and Bias Blue: Bias Brown: Variance Yellow: Training set MSE Green: Testing set MSE Black: Irreducibile error (Bayes) (b) Explain why each of the five curves has the shape displayed in part (a). Yellow: As models become more flexible and complex, the training error is reduced, but with diminishing benefit. Green: We often observe a U-shaped error in the holdout training error. This is a combination of the Bias error above the least flexible model and the Variance error from over-fitting the most flexible models and the irreducible error component. Blue: The bias error component, as observed in the test result Brown: The variance error component, as observed in the test result Black: The irreducible error component, as observed in the test result 4. You will now think of some real-life applications for statistical learning. (a) Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer. Infer the causes of expedited freight for a component part. The independent predictors would include country of origin, shipping method, promised lead time, and schedule changes for a year of deliveries. Predict customer churn for a learning labs subscription education program. The independent predictors would include monthly payment history, course topics, and web site visits. Predict the alphabetic letter of samples of handwriting. The independent predictors would be the pixels of many labeled images. (b) Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer. Predict the survival rate of a vehicle component part in use by customers. The response is the proportion of parts still in service, censored. The independent predictors would include engine hours accumulated, region, machine chassis, and type of farm. Infer the characteristics of bottles of wine that influence price. The independent predictors would include the vintage year, the type of grape, the region, the color of the bottle, and the length of the story on the label. Predict the number of sales conversions per visitor for a web portal store front, given the catalog of products available, the layout parameters of the web site, the colors, and dimensions of the shopping experience. (c) Describe three real-life applications in which cluster analysis might be useful. Given demographic data on millions of customers and what they buy, build five persona’s for describing the current customer base to a company investor. Given IoT operating data from agricultural equipment, build three persona’s for characterizing duty cycles of a fleet. Given employee communications transactional data, build three persona’s for better tailoring targeted leadership training. 5. What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred? The very flexible GBMs and Neural Nets require more data, and are often both less interpretable and explainable. Banks and highly regulated entities prefer simple linear models and decision trees so they can explain their management policies in simple terms. 6. Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a nonparametric approach)? What are its disadvantages? In any parametric approach we start with an assumption about the functional form and then work towards fitting the data to the closest version of that functional form. On advantage is in explainability, and another is that less data is required to build a useful model. A disadvantage is that the model might never achieve the lowest error rate. 7. The table below provides a training data set containing six observations, three predictors, and one qualitative response variable. Obs. X1 X2 X3 Y 1 0 3 0 Red 2 2 0 0 Red 3 0 1 3 Red 4 0 1 2 Green 5 −1 0 1 Green 6 1 1 1 Red Suppose we wish to use this data set to make a prediction for Y when X1 = X2 = X3 = 0 using K-nearest neighbors. (a) Compute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0. Red: sqrt((0-0)^2 + (0-3)^2 + (0-0)^2 )= sqrt(9) = 3 Red: sqrt((0-2)^2 + (0-0)^2 + (0-0)^2 )= sqrt(4) = 2 Red: sqrt((0-0)^2 + (0-1)^2 + (0-3)^2 )= sqrt(10) = 3.162278 Green: sqrt((0-0)^2 + (0-1)^2 + (0-2)^2 )= sqrt(5)= 2.236068 Green: sqrt((0+1)^2 + (0-0)^2 + (0-1)^2 )= sqrt(2) =1.414214 Red: sqrt((0-1)^2 + (0-1)^2 + (0-1)^2 )= sqrt(3)=1.732051 (b) What is our prediction with K = 1? Why? For a test set X1=X2=X3=0, this is closest to Green which is at a distance sqrt(2). Therefore the prediction is Green. (c) What is our prediction with K = 3? Why? For a test set X1=X2=X3=0, this is closest to Red (Obs 2), Green(Obs 5) and Red(Obs 6). Thus the prediction will be Red. (d) If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why? Small k values yield a model with lots of detailed curves in the boundary, and likely the lowest irreducible error. Applied 8. This exercise relates to the College data set, which can be found in the file College.csv on the book website. (a) Use the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data. (b) Look at the data using the View() function. You should notice that the first column is just the name of each university. We don’t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands: (c) i. Use the summary() function to produce a numerical summary of the variables in the data set. college &lt;- read_csv(&quot;https://www.statlearning.com/s/College.csv&quot;, show_col_types = FALSE) %&gt;% rename(college = `...1`) ## New names: ## * `` -&gt; ...1 summary(college) ## college Private Apps Accept ## Length:777 Length:777 Min. : 81 Min. : 72 ## Class :character Class :character 1st Qu.: 776 1st Qu.: 604 ## Mode :character Mode :character Median : 1558 Median : 1110 ## Mean : 3002 Mean : 2019 ## 3rd Qu.: 3624 3rd Qu.: 2424 ## Max. :48094 Max. :26330 ## Enroll Top10perc Top25perc F.Undergrad ## Min. : 35 Min. : 1.00 Min. : 9.0 Min. : 139 ## 1st Qu.: 242 1st Qu.:15.00 1st Qu.: 41.0 1st Qu.: 992 ## Median : 434 Median :23.00 Median : 54.0 Median : 1707 ## Mean : 780 Mean :27.56 Mean : 55.8 Mean : 3700 ## 3rd Qu.: 902 3rd Qu.:35.00 3rd Qu.: 69.0 3rd Qu.: 4005 ## Max. :6392 Max. :96.00 Max. :100.0 Max. :31643 ## P.Undergrad Outstate Room.Board Books ## Min. : 1.0 Min. : 2340 Min. :1780 Min. : 96.0 ## 1st Qu.: 95.0 1st Qu.: 7320 1st Qu.:3597 1st Qu.: 470.0 ## Median : 353.0 Median : 9990 Median :4200 Median : 500.0 ## Mean : 855.3 Mean :10441 Mean :4358 Mean : 549.4 ## 3rd Qu.: 967.0 3rd Qu.:12925 3rd Qu.:5050 3rd Qu.: 600.0 ## Max. :21836.0 Max. :21700 Max. :8124 Max. :2340.0 ## Personal PhD Terminal S.F.Ratio ## Min. : 250 Min. : 8.00 Min. : 24.0 Min. : 2.50 ## 1st Qu.: 850 1st Qu.: 62.00 1st Qu.: 71.0 1st Qu.:11.50 ## Median :1200 Median : 75.00 Median : 82.0 Median :13.60 ## Mean :1341 Mean : 72.66 Mean : 79.7 Mean :14.09 ## 3rd Qu.:1700 3rd Qu.: 85.00 3rd Qu.: 92.0 3rd Qu.:16.50 ## Max. :6800 Max. :103.00 Max. :100.0 Max. :39.80 ## perc.alumni Expend Grad.Rate ## Min. : 0.00 Min. : 3186 Min. : 10.00 ## 1st Qu.:13.00 1st Qu.: 6751 1st Qu.: 53.00 ## Median :21.00 Median : 8377 Median : 65.00 ## Mean :22.74 Mean : 9660 Mean : 65.46 ## 3rd Qu.:31.00 3rd Qu.:10830 3rd Qu.: 78.00 ## Max. :64.00 Max. :56233 Max. :118.00 ii. Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[,1:10]. # pairs(college[,3:12]) GGally::ggpairs(college[,2:11], mapping = aes(color = Private), progress = FALSE, lower = list(combo = GGally::wrap(&quot;facethist&quot;, bins = 40))) + theme_bw() + theme(panel.grid = element_blank()) + labs(caption = &quot;Source: ISLR2::College | Ten numeric features&quot;) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 iii. Use the plot() function to produce side-by-side boxplots of Outstate versus Private. # plot( college$Private, college$Outstate ) college %&gt;% ggplot(aes(Private, Outstate)) + geom_boxplot() + theme_bw() + theme(panel.grid = element_blank()) + labs(caption = &quot;Source: ISLR2::College&quot;) iv. Create a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10 % of their high school classes exceeds 50 %. Use the summary() function to see how many elite universities there are. # Elite &lt;- rep(&quot;No&quot;, nrow(college)) # Elite[college$Top10perc &gt; 50] &lt;- &quot; Yes &quot; # Elite &lt;- as.factor(Elite) # college &lt;- data.frame(college , Elite) college &lt;- college %&gt;% mutate(Elite = as_factor(if_else(Top10perc &gt; 50, &quot;Yes&quot;, &quot;No&quot;))) summary(college$Elite) ## No Yes ## 699 78 Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite. #plot( college$Elite, college$Outstate ) college %&gt;% ggplot(aes(Elite, Outstate)) + geom_boxplot() + theme_bw() + theme(panel.grid = element_blank()) + labs(caption = &quot;Source: ISLR2::College&quot;) v. Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow = c(2, 2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways. college %&gt;% ggplot(aes(Enroll, fill = Private)) + geom_histogram(bins = 40) + theme_bw() + theme(panel.grid = element_blank()) + labs(caption = &quot;Source: ISLR2::College&quot;) vi. Continue exploring the data, and provide a brief summary of what you discover. GGally::ggpairs(bind_cols(college[,2],college[,12:18]), mapping = aes(color = Private), progress = FALSE, lower = list(combo = GGally::wrap(&quot;facethist&quot;, bins = 40))) + theme_bw() + theme(panel.grid = element_blank()) + labs(caption = &quot;Source: ISLR2::College | Ten numeric features&quot;) The public institutions in this dataset enroll more undergraduates, while the private ones have more pursuing terminal degrees in their fields. 9. This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data. (a) Which of the predictors are quantitative, and which are qualitative? Auto &lt;- ISLR2::Auto %&gt;% drop_na %&gt;% tibble() # summary(Auto) skimr::skim(Auto) Table 2.1: Data summary Name Auto Number of rows 392 Number of columns 9 _______________________ Column type frequency: factor 1 numeric 8 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts name 0 1 FALSE 301 amc: 5, for: 5, toy: 5, amc: 4 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist mpg 0 1 23.45 7.81 9 17.00 22.75 29.00 46.6 ▆▇▆▃▁ cylinders 0 1 5.47 1.71 3 4.00 4.00 8.00 8.0 ▇▁▃▁▅ displacement 0 1 194.41 104.64 68 105.00 151.00 275.75 455.0 ▇▂▂▃▁ horsepower 0 1 104.47 38.49 46 75.00 93.50 126.00 230.0 ▆▇▃▁▁ weight 0 1 2977.58 849.40 1613 2225.25 2803.50 3614.75 5140.0 ▇▇▅▅▂ acceleration 0 1 15.54 2.76 8 13.78 15.50 17.02 24.8 ▁▆▇▂▁ year 0 1 75.98 3.68 70 73.00 76.00 79.00 82.0 ▇▆▇▆▇ origin 0 1 1.58 0.81 1 1.00 1.00 2.00 3.0 ▇▁▂▁▂ Quantitative: mpg, cylinders, displacement, horsepower, weight, acceleration, year, origin Qualitative: name (b) What is the range of each quantitative predictor? You can answer this using the range() function. range() Auto %&gt;% select(where(is.numeric)) %&gt;% summarize(across(everything(), range)) %&gt;% mutate(level = c(&quot;min&quot;, &quot;max&quot;)) %&gt;% pivot_longer(cols = -level, names_to = &quot;metric&quot;, values_to = &quot;value&quot;) %&gt;% pivot_wider(names_from = level, values_from = value) ## # A tibble: 8 × 3 ## metric min max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mpg 9 46.6 ## 2 cylinders 3 8 ## 3 displacement 68 455 ## 4 horsepower 46 230 ## 5 weight 1613 5140 ## 6 acceleration 8 24.8 ## 7 year 70 82 ## 8 origin 1 3 (c) What is the mean and standard deviation of each quantitative predictor? Auto %&gt;% select(where(is.numeric)) %&gt;% summarize(across(everything(), mean, na.rm = TRUE)) %&gt;% pivot_longer(everything(), values_to = &quot;mean&quot;) ## # A tibble: 8 × 2 ## name mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 mpg 23.4 ## 2 cylinders 5.47 ## 3 displacement 194. ## 4 horsepower 104. ## 5 weight 2978. ## 6 acceleration 15.5 ## 7 year 76.0 ## 8 origin 1.58 Auto %&gt;% select(where(is.numeric)) %&gt;% summarize(across(everything(), sd, na.rm = TRUE)) %&gt;% pivot_longer(everything(), values_to = &quot;sd&quot;) ## # A tibble: 8 × 2 ## name sd ## &lt;chr&gt; &lt;dbl&gt; ## 1 mpg 7.81 ## 2 cylinders 1.71 ## 3 displacement 105. ## 4 horsepower 38.5 ## 5 weight 849. ## 6 acceleration 2.76 ## 7 year 3.68 ## 8 origin 0.806 (d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains? Auto[-c(10:85), ] %&gt;% select(where(is.numeric)) %&gt;% summarize(across(everything(), range, na.rm = TRUE)) %&gt;% mutate(level = c(&quot;min&quot;, &quot;max&quot;)) %&gt;% pivot_longer(cols = -level, names_to = &quot;metric&quot;, values_to = &quot;value&quot;) %&gt;% pivot_wider(names_from = level, values_from = value) ## # A tibble: 8 × 3 ## metric min max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mpg 11 46.6 ## 2 cylinders 3 8 ## 3 displacement 68 455 ## 4 horsepower 46 230 ## 5 weight 1649 4997 ## 6 acceleration 8.5 24.8 ## 7 year 70 82 ## 8 origin 1 3 Auto[-c(10:85), ] %&gt;% select(where(is.numeric)) %&gt;% summarize(across(everything(), mean, na.rm = TRUE)) %&gt;% pivot_longer(everything(), values_to = &quot;mean&quot;) ## # A tibble: 8 × 2 ## name mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 mpg 24.4 ## 2 cylinders 5.37 ## 3 displacement 187. ## 4 horsepower 101. ## 5 weight 2936. ## 6 acceleration 15.7 ## 7 year 77.1 ## 8 origin 1.60 Auto[-c(10:85), ] %&gt;% select(where(is.numeric)) %&gt;% summarize(across(everything(), sd, na.rm = TRUE)) %&gt;% pivot_longer(everything(), values_to = &quot;sd&quot;) ## # A tibble: 8 × 2 ## name sd ## &lt;chr&gt; &lt;dbl&gt; ## 1 mpg 7.87 ## 2 cylinders 1.65 ## 3 displacement 99.7 ## 4 horsepower 35.7 ## 5 weight 811. ## 6 acceleration 2.69 ## 7 year 3.11 ## 8 origin 0.820 (e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings. GGally::ggpairs(select(Auto, -name), progress = FALSE, lower = list(combo = GGally::wrap(&quot;facethist&quot;, bins = 40))) displacement appears to have a positive relationship with horsepower mpg has a negative, nonlinear relationship with displacement (f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer. Yes, a predictive model is certainly possible with this data set. 2.4 Exercises 57 10. This exercise involves the Boston housing data set. (a) To begin, load in the Boston data set. The Boston data set is part of the ISLR2 library. How many rows are in this data set? nrow(ISLR2::Boston) ## [1] 506 How many columns? ncol(ISLR2::Boston) ## [1] 13 What do the rows and columns represent? These are housing sale values medv and the characteristics of 506 suburbs of Boston. (b) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings. GGally::ggpairs(ISLR2::Boston, progress = FALSE, lower = list(combo = GGally::wrap(&quot;facethist&quot;, bins = 40))) medv is reduced with higher lstat medv increases with rm (c) Are any of the predictors associated with per capita crime rate? If so, explain the relationship. Yes, rad highway access moderately at 0.626. Crime is highest at low rad, drops quickly, and then increase somewhat at high rad. (d) Do any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor ISLR2::Boston %&gt;% slice_max(crim, n = 3) ## crim zn indus chas nox rm age dis rad tax ptratio lstat medv ## 1 88.9762 0 18.1 0 0.671 6.968 91.9 1.4165 24 666 20.2 17.21 10.4 ## 2 73.5341 0 18.1 0 0.679 5.957 100.0 1.8026 24 666 20.2 20.62 8.8 ## 3 67.9208 0 18.1 0 0.693 5.683 100.0 1.4254 24 666 20.2 22.98 5.0 ISLR2::Boston %&gt;% ggplot(aes(crim)) + geom_histogram(bins = 30) Yes, the median crime rate crim is very low, at only 0.257. This feature is heavily skewed right. The range runs from 0.00637 to 89.0. ISLR2::Boston %&gt;% slice_max(tax, n = 3) ## crim zn indus chas nox rm age dis rad tax ptratio lstat medv ## 1 0.15086 0 27.74 0 0.609 5.454 92.7 1.8209 4 711 20.1 18.06 15.2 ## 2 0.18337 0 27.74 0 0.609 5.414 98.3 1.7554 4 711 20.1 23.97 7.0 ## 3 0.20746 0 27.74 0 0.609 5.093 98.0 1.8226 4 711 20.1 29.68 8.1 ## 4 0.10574 0 27.74 0 0.609 5.983 98.8 1.8681 4 711 20.1 18.07 13.6 ## 5 0.11132 0 27.74 0 0.609 5.983 83.5 2.1099 4 711 20.1 13.35 20.1 ISLR2::Boston %&gt;% ggplot(aes(tax)) + geom_histogram(bins = 30) Yes, median full value tax rates range from 169k to 711k. The distribution across suburbs is also not even. ISLR2::Boston %&gt;% slice_min(ptratio , n = 3) ## crim zn indus chas nox rm age dis rad tax ptratio lstat medv ## 1 0.04011 80 1.52 0 0.404 7.287 34.1 7.309 2 329 12.6 4.08 33.3 ## 2 0.04666 80 1.52 0 0.404 7.107 36.6 7.309 2 329 12.6 8.61 30.3 ## 3 0.03768 80 1.52 0 0.404 7.274 38.3 7.309 2 329 12.6 6.62 34.6 ISLR2::Boston %&gt;% ggplot(aes(ptratio)) + geom_histogram(bins = 30) The ptratio is skewed left, with the bulk of the suburbs at the high end of the range from 2.16 to 38. (e) How many of the census tracts in this data set bound the Charles river? ISLR2::Boston %&gt;% count(chas == 1) ## chas == 1 n ## 1 FALSE 471 ## 2 TRUE 35 35 suburbs bound the Charles River (f) What is the median pupil-teacher ratio among the towns in this data set? ISLR2::Boston %&gt;% summarise(ptratio_median = median(ptratio)) ## ptratio_median ## 1 19.05 (g) Which census tract of Boston has lowest median value of owner occupied homes? ISLR2::Boston %&gt;% add_rowindex() %&gt;% select(.row, medv) %&gt;% filter(medv == min(medv)) ## .row medv ## 1 399 5 ## 2 406 5 Rows 399 and 406. What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings. ISLR2::Boston %&gt;% mutate(bottom_tracts = if_else(medv == min(medv), &quot;Bottom&quot;, NA_character_)) %&gt;% pivot_longer(cols = -c(bottom_tracts, medv)) %&gt;% ggplot(aes(value, medv, color = bottom_tracts)) + geom_point() + facet_wrap(vars(name), scales = &quot;free_x&quot;) + theme_bw() + theme(strip.placement = &quot;outside&quot;, panel.grid = element_blank(), strip.background = element_blank()) + labs(caption = &quot;Source: ISLR2::Boston | Lowest Median Home Value Suburbs&quot;, color = NULL) The lowest median value suburbs of Boston are the oldest, they have higher crime than most, they are close to empoyment centers, they are related to industry, they have high taxes, and a low zn proportion of large lots zoned. (h) In this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling. ISLR2::Boston %&gt;% mutate(room_state = case_when( rm &gt; 8 ~ &quot;eight_plus&quot;, rm &gt; 7 ~ &quot;seven_to_eight&quot;, TRUE ~ NA_character_ )) %&gt;% count(room_state) ## room_state n ## 1 eight_plus 13 ## 2 seven_to_eight 51 ## 3 &lt;NA&gt; 442 ISLR2::Boston %&gt;% mutate(room_state = case_when( rm &gt; 8 ~ &quot;eight_plus&quot;, rm &gt; 7 ~ &quot;seven_to_eight&quot;, TRUE ~ NA_character_ )) %&gt;% pivot_longer(cols = -c(room_state, medv)) %&gt;% ggplot(aes(value, medv, color = room_state)) + geom_point(shape = 21, alpha = 0.7) + facet_wrap(vars(name), scales = &quot;free_x&quot;) + theme_bw() + theme(strip.placement = &quot;outside&quot;, panel.grid = element_blank(), strip.background = element_blank()) + labs(caption = &quot;Source: ISLR2::Boston | More than 8 rooms per dwelling&quot;, color = NULL) Again, the oldest homes have more than 8 rooms per dwelling. They have the lowest tax assessments and a low proportion of non-retail business acres per town. "],["linear.html", "Chapter 3 Linear Regression", " Chapter 3 Linear Regression Learning objectives: Perform linear regression with a single predictor variable. Estimate the standard error of regression coefficients. Evaluate the goodness of fit of a regression. Perform linear regression with multiple predictor variables. Evaluate the relative importance of variables in a multiple linear regression. Include interaction effects in a multiple linear regression. Perform linear regression with qualitative predictor variables. Model non-linear relationships using polynomial regression. Identify non-linearity in a data set. Compare and contrast linear regression with KNN regression. "],["questions-to-answer.html", "3.1 Questions to Answer", " 3.1 Questions to Answer Recall the Advertising data from Chapter 2. Here are a few important questions that we might seek to address: Is there a relationship between advertising budget and sales? How strong is the relationship between advertising budget and sales? Does knowledge of the advertising budget provide a lot of information about product sales? Which media are associated with sales? How large is the association between each medium and sales? For every dollar spent on advertising in a particular medium, by what amount will sales increase? How accurately can we predict future sales? Is the relationship linear? If there is approximately a straight-line relationship between advertising expenditure in the various media and sales, then linear regression is an appropriate tool. If not, then it may still be possible to transform the predictor or the response so that linear regression can be used. Is there synergy among the advertising media? Or, in stats terms, is there an interaction effect? "],["simple-linear-regression-definition.html", "3.2 Simple Linear Regression: Definition", " 3.2 Simple Linear Regression: Definition Simple linear regression: Very straightforward approach to predicting response \\(Y\\) on predictor \\(X\\). \\[Y \\approx \\beta_{0} + \\beta_{1}X\\] Read “\\(\\approx\\)” as “is approximately modeled by.” \\(\\beta_{0}\\) = intercept \\(\\beta_{1}\\) = slope \\[\\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x\\] \\(\\hat{\\beta}_{0}\\) = our approximation of intercept \\(\\hat{\\beta}_{1}\\) = our approximation of slope \\(x\\) = sample of \\(X\\) \\(\\hat{y}\\) = our prediction of \\(Y\\) from \\(x\\) "],["simple-linear-regression-visualization.html", "3.3 Simple Linear Regression: Visualization", " 3.3 Simple Linear Regression: Visualization Figure 3.1: For the Advertising data, the least squares fit for the regression of sales onto TV is shown. The fit is found by minimizing the residual sum of squares. Each grey line segment represents a residual. In this case a linear fit captures the essence of the relationship, although it overestimates the trend in the left of the plot. "],["simple-linear-regression-math.html", "3.4 Simple Linear Regression: Math", " 3.4 Simple Linear Regression: Math RSS = residual sum of squares \\[\\mathrm{RSS} = e^{2}_{1} + e^{2}_{2} + \\ldots + e^{2}_{n}\\] \\[\\mathrm{RSS} = (y_{1} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{1})^{2} + (y_{2} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{2})^{2} + \\ldots + (y_{n} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{n})^{2}\\] \\[\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n}{(x_{i}-\\bar{x})(y_{i}-\\bar{y})}}{\\sum_{i=1}^{n}{(x_{i}-\\bar{x})^{2}}}\\] \\[\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}\\] \\(\\bar{x}\\), \\(\\bar{y}\\) = sample means of \\(x\\) and \\(y\\) 3.4.1 Visualization of Fit Figure 3.2: Contour and three-dimensional plots of the RSS on the Advertising data, using sales as the response and TV as the predictor. The red dots correspond to the least squares estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\), given by (3.4) Learning Objectives: Perform linear regression with a single predictor variable. ✔️ "],["assessing-accuracy-of-coefficient-estimates.html", "3.5 Assessing Accuracy of Coefficient Estimates", " 3.5 Assessing Accuracy of Coefficient Estimates \\[Y = \\beta_{0} + \\beta_{1}X + \\epsilon\\] RSE = residual standard error Estimate of \\(\\sigma\\) \\[\\mathrm{RSE} = \\sqrt{\\frac{\\mathrm{RSS}}{n - 2}}\\] \\[\\mathrm{SE}(\\hat\\beta_0)^2 = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\right],\\ \\ \\mathrm{SE}(\\hat\\beta_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\] 95% confidence interval: a range of values such that with 95% probability, the range will contain the true unknown value of the parameter If we take repeated samples and construct the confidence interval for each sample, 95% of the intervals will contain the true unknown value of the parameter \\[\\hat\\beta_1 \\pm 2\\ \\cdot \\ \\mathrm{SE}(\\hat\\beta_1)\\] \\[\\hat\\beta_0 \\pm 2\\ \\cdot \\ \\mathrm{SE}(\\hat\\beta_0)\\] Learning Objectives: Estimate the standard error of regression coefficients. ✔️ "],["meeting-videos-1.html", "3.6 Meeting Videos", " 3.6 Meeting Videos 3.6.1 Cohort 1 Meeting chat log ADD LOG HERE "],["classification.html", "Chapter 4 Classification", " Chapter 4 Classification Learning objectives: Compare and contrast classification with linear regression. Perform classification using logistic regression. Perform classification using linear discriminant analysis (LDA). Perform classification using quadratic discriminant analysis (QDA). Perform classification using naive Bayes. Identify the strengths and weaknesses of the various classification models. Model count data using Poisson regression. "],["an-overview-of-classification.html", "4.1 An Overview of Classification", " 4.1 An Overview of Classification Classification: Approaches to make inference and/or predict qualitative (categorical) response variable Few common classification techniques (classifiers): logistic regression linear discriminant analysis (LDA) quadratic discriminant analysis (QDA) naive Bayes K-nearest neighbors - Examples of classification problems: A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have? Predictor variable: Symptoms Response variable: Type of medical conditions An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP address, past transaction history, and so forth. Predictor variable: User’s IP address, past transaction history, etc Response variable: Fraudulent activity (Yes/No) On the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not. Predictor variable: DNA sequence data Response variable: Presence of deleterious gene (Yes/No) In the following section, we are going to explore the Default dataset. The annual incomes (\\(X_1\\) = income) and monthly credit card balances (\\(X_2\\) =balance) are used to predict whether whether an individual will default on his or her credit card payment. Figure 4.1: The distribution of balance and income split by the binary default variable respectively; Note. Defaulters represented as orange plus sign; non-defaulters represented as blue circle "],["why-not-linear-regression.html", "4.2 Why NOT Linear Regression?", " 4.2 Why NOT Linear Regression? a regression method cannot convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression \\[Y = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if stroke};\\\\ 2 &amp; \\mbox{if epileptic seizure};\\\\ 3 &amp; \\mbox{if drug overdose}.\\end{array} \\right.\\] a regression method will not provide meaningful estimates of Pr(Y |X), even with just two classes; partial estimates might be outside the [0, 1] probability interval "],["logistic-regression.html", "4.3 Logistic Regression", " 4.3 Logistic Regression Logistic regression: models the probability that Y belongs to a particular category (X) X is binary (0/1) \\[p(X) = β_0 + β_1X \\space \\Longrightarrow {Linear \\space regression}\\] \\[p (X) = \\frac{e^{\\beta_{0} + \\beta_{1}X}}{1 + e^{\\beta_{0} + \\beta_{1}X}} \\space \\Longrightarrow {Logistic \\space function}\\] \\[odds = p (X) = \\frac{e^{\\beta_{0} + \\beta_{1}X}}{1 + e^{\\beta_{0} + \\beta_{1}X}} \\Longrightarrow {odds \\space value [0, ∞]}\\] \\[\\log \\biggl(\\frac{p(X)}{1- p(X)}\\bigg) = \\beta_{0} + \\beta_{1}X \\Longrightarrow {log \\space odds/logit}\\] By logging the whole equation, we get \\[\\log \\biggl(\\frac{p(X)}{1- p(X)}\\bigg) = \\beta_{0} + \\beta_{1}X \\Longrightarrow {log \\space odds/logit}\\] To estimate the regression coefficient, we use maximum likelihood (ME). Likelihood Function \\[ℓ (\\beta_{0}, \\beta_{1}) = \\prod_{i: y_{i}= 1} p (x_i) = \\prod_{i&#39;: y_{i&#39;}= 0} (1- p (x_{i&#39;})) \\Longrightarrow {Likelihood \\space function}\\] 4.3.1 Multiple Logistic Regression \\[\\log \\biggl(\\frac{p(X)}{1- p(X)}\\bigg) = \\beta_{0} + \\beta_{1}X_1 + ... + \\beta_{p}X_p \\\\ \\Downarrow \\\\ p(X) = \\frac{e^{\\beta_{0} + \\beta_{1}X_1 + ... + \\beta_{p}X_p}}{1 + \\beta_{0} + \\beta_{1}X_1 + ... + \\beta_{p}X_p}\\] Figure 4.2: Confounding in the Default data. Left: Default rates are shown for students (orange) and non-students (blue). The solid lines display default rate as a function of balance, while the horizontal broken lines display the overall default rates. Right: Boxplots of balance for students (orange) and non-students (blue) are shown. 4.3.2 Multinomial Logistic Regression This is used in the setting where K &gt; 2 classes. In multinomial, we select a single class to serve as the baseline. However, the interpretation of the coefficients in a multinomial logistic regression model must be done with care, since it is tied to the choice of baseline. Alternatively, you can use `Softmax coding, where we treat all K classes symmetrically, and assume that for k = 1, . . . ,K, rather than selecting a baseline. This means, we estimate coefficients for all K classes, rather than estimating coefficients for K − 1 classes. "],["generative-models-for-classification.html", "4.4 Generative Models for Classification", " 4.4 Generative Models for Classification Why Logistic Regression is not ideal? When there is substantial separation between the two classes, the parameter estimates for the logistic regression model are surprisingly unstable. If the distribution of the predictors X is approximately normal in each of the classes and the sample size is small, then the generative modelling may be more accurate than logistic regression. Generative modelling can be naturally extended to the case of more than two response classes. Common notations: - K \\(\\Longrightarrow\\) response class \\(π_k \\Longrightarrow\\) overall or prior probability that a randomly chosen observation comes from the prior kth class; can be obtained from the random sample from the population \\(f_k(X) ≡ Pr(X|Y = k)^1 \\Longrightarrow\\) the density function of X density for an observation that comes from the kth class; requires some underlying assumption to estimate Bayes’ theorem states that \\[Pr(Y = k|X = x) = \\frac {π_k f_k(x)}{\\sum_{l =1}^{k} π_lf_l(x)}\\] \\(p_k(x) = Pr(Y = k|X = x) \\Longrightarrow\\) posterior probability that an observation posterior X = x belongs to the kth class; computed from \\(f_k(X)\\) "],["a-comparison-of-classification-methods.html", "4.5 A Comparison of Classification Methods", " 4.5 A Comparison of Classification Methods Each of the classifiers below uses different estimates of \\(f_k(x)\\). linear discriminant analysis; quadratic discriminant analysis; naive Bayes 4.5.1 Linear Discriminant Analysis for p = 1 one predictor classify an observation to the class for which \\(p_k(x)\\) is greatest Assumptions: - we assume that \\(f_k(x)\\) is normal or Gaussian with a classs pecific mean and, - a shared variance term across all K classes [\\(σ^2_1 = · · · = σ^2_K\\) ] The normal density takes the form \\[f_k(x) = \\frac{1}{\\sqrt{2πσk}}exp(- \\frac{1}{2σ^2_k}(x- \\mu_k)^2)\\] Then, the posterior probability (probability that the observation belongs to the kth class, given the predictor value for that observation) is \\[p_k(x) = \\frac{π_k \\frac{1}{\\sqrt{2πσk}}exp(- \\frac{1}{2σ^2_k}(x- \\mu_k)^2)}{\\sum^k_{l=1} π_l \\frac{1}{\\sqrt{2πσk}}exp(- \\frac{1}{2σ^2_k}(x- \\mu_l)^2)}\\] Additional mathematical formula After you log and rearrange the above equation, you will the following formula. The Bayes’ classifier assign to one class if \\(2x (μ_1 − μ_2) &gt; μ_1^2 − μ_2^2\\) and otherwise. \\[δ_k(x) = x . \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + log(π_k) \\Longrightarrow {Equation \\space 4.18}\\] The Bayes decision boundary is the point for which \\(δ_1(x) = δ_2(x)\\) \\[x = \\frac{μ_1^2 − μ_2^2}{2(μ_1 − μ_2)} = \\frac{μ_1 + μ_2}{2}\\] Figure 4.3: Left: Two one-dimensional normal density functions are shown. The dashed vertical line represents the Bayes decision boundary. Right: 20 observations were drawn from each of the two classes, and are shown as histograms. The Bayes decision boundary is again shown as a dashed vertical line. The solid vertical line represents the LDA decision boundary estimated from the training data. The linear discriminant analysis (LDA) method approximates the linear discriminant analysis Bayes classifier by plugging estimates for \\(π_k\\), \\(μ_k\\), and σ^2 into equation 4.18. \\(\\hat μ_k\\) is the average of all the training observations from the kth class \\[\\hat{\\mu}_{k} = \\frac{1}{n_{k}}\\sum_{i: y_{i}= k} x_{i}\\] \\(\\hat σ^2\\) is the weighted average of the sample variances for each of the K classes \\[\\hat{\\sigma}^2 = \\frac{1}{n - K} \\sum_{k = 1}^{K} \\sum_{i: y_{i}= k} (x_{i} - \\hat{\\mu}_{k})^2\\] Note. n = total number of training observations, \\(n_k\\) = number of training observations in the kth class \\(π_k\\) is estimated from the proportion of the training observations that belong to the kth class. \\(π_k = \\frac{n_k}{n}\\) LDA classifier assigns an observation X = x to the class for which \\(δ_k(x)\\) is largest. \\[δ_k(x) = x . \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + log(π_k) \\Longrightarrow {Equation \\space 4.18} \\\\ \\Downarrow \\\\ \\hat δ_k(x) = x \\cdot \\frac{\\hat \\mu_k}{\\hat \\sigma^2} - \\frac{\\hat \\mu_k^2}{2\\hat \\sigma^2} + log(\\hat π_k)\\] 4.5.2 Linear Discriminant Analysis for p &gt; 1 multiple predictors; p &gt; 1 predictors observations come from a multivariate Gaussian (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix; \\[N(μ_k,Σ)\\] Assumptions: - each individual predictor follows a one-dimensional normal distribution, with predictors having some correlation Figure 4.4: Two multivariate Gaussian density functions are shown, with p = 2. Left: The two predictors are uncorrelated and it has a circular base. Var(X_1) = Var(X_2) and Cor(X_1,X_2) = 0; Right: The two variables have a correlation of 0.7 with a elliptical base \\(\\exp\\) The multivariate Gaussian density is defined as: \\[f(x) = \\frac{1}{(2π)^{\\frac{p}{2}}|Σ|^{\\frac{1}{2}}}\\exp -\\frac{1}{2}(x - \\mu)^T Σ^{−1}(x − μ))\\] Bayes classifier assigns an observation X = x to the class for which \\[δ_k(x)\\] is largest. \\[δ_k(x) = x^T Σ^{−1}μ_k - \\frac{1}{2}μ_k^T Σ^{−1} μ_k + log π_k \\Longrightarrow vector/matrix \\space version \\\\ δ_k(x) = x . \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + log(π_k) \\Longrightarrow {Equation \\space 4.18}\\] Figure 4.5: An example with three classes. The observations from each class are drawn from a multivariate Gaussian distribution with p = 2, with a class-specific mean vector and a common covariance matrix. Left: Ellipses that contain 95% of the probability for each of the three classes are shown. The dashed lines are the Bayes decision boundaries. Right: 20 observations were generated from each class, and the corresponding LDA decision boundaries are indicated using solid black lines. The Bayes decision boundaries are once again shown as dashed lines. Overall, the LDA decision boundaries are pretty close to the Bayes decision boundaries, shown again as dashed lines. The test error rates for the Bayes and LDA classifiers are 0.0746 and 0.0770, respectively. All classification models have training error rate, which can be displayed with a confusion matrix. Caveats of error rate: training error rates will usually be lower than test error rates, which are the real quantity of interest. The higher the ratio of parameters p to number of samples n, the more we expect this overfitting to play a role. the trivial null classifier will achieve an error rate that is only a bit higher than the LDA training set error rate a binary classifier such as this one can make two types of errors (Type I and II) Class-specific performance (sensitivity and specificity) is important in certain fields (e.g., medicine) LDA has low sensitivity due to 1. LDA is trying to approximate the Bayes classifier, which has the lowest total error rate out of all classifiers 2. In the process, the Bayes classifier will yield the smallest possible total number of misclassified observations, regardless of the class from which the errors stem. 3. It also uses a threshold of 50% for the posterior probability of default in order to assign an observation to the default class \\[Pr(default = Yes|X = x) &gt; 0.5. \\\\ Pr(default = Yes|X = x) &gt; 0.2.\\] Figure 4.6: The figure illustrates the trade-off that results from modifying the threshold value for the posterior probability of default. For the Default data set, error rates are shown as a function of the threshold value for the posterior probability that is used to perform the assignment. The black solid line displays the overall error rate. The blue dashed line represents the fraction of defaulting customers that are incorrectly classified, and the orange dotted line indicates the fraction of errors among the non-defaulting customers. As the threshold is reduced, the error rate among individuals who default decreases steadily, but the error rate among the individuals who do not default increases. The decision on the threshold must be based on domain knowledge (e.g., detailed information about the costs associated with default) ROC curve is a way to illustrate the two type of errors at all possible thresholds. Figure 4.7: The true positive rate is the sensitivity: the fraction of defaulters that are correctly identified, using a given threshold value. The false positive rate is 1-specificity: the fraction of non-defaulters that we classify incorrectly as defaulters, using that same threshold value. The ideal ROC curve hugs the top left corner, indicating a high true positive rate and a low false positive rate. The dotted line represents the “no information” classifier; this is what we would expect if student status and credit card balance are not associated with probability of default. An ideal ROC curve will hug the top left corner, so the larger area under the ROC curve (AUC), the better the classifier. True class Neg. or Null Pos. or Non-null Total Predicted class − or Null True Neg. (TN) False Neg. (FN) N∗ + or Non-null False Pos. (FP) True Pos. (TP) P∗ Total N P Important measures for classification and diagnostic testing: False Positive rate (FP/N) \\(\\Longrightarrow\\) Type I error, 1−Specificity True Positive rate (TP/P) \\(\\Longrightarrow\\) 1−Type II error, power, sensitivity, recall Pos. Predicted value (TP/P∗) \\(\\Longrightarrow\\) Precision, 1−false discovery proportion Neg. Predicted value (TN/N∗) 4.5.3 Quadratic Discriminant Analysis (QDA) Assumptions similar to LDA, in which observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to perform prediction QDA assumes that each class has its own covariance matrix \\[X ∼ N(μ_k,Σ_k) \\Longrightarrow {Σ_k is \\space covariance \\space matrix \\space for \\space the \\space kth \\space class}\\] Bayes classifier \\[δ_k(x) = - \\frac{1}{2}(x - \\mu_k)^T Σ_k^{−1}(x - \\mu_k) - \\frac{1}{2}log|Σ_k| + log(π_k) \\\\ \\Downarrow \\\\ δ_k(x) = - \\frac{1}{2}x^T Σ_k^{−1}x - x^T Σ_k^{−1} \\mu_k - \\frac{1}{2}μ_k^T Σ_k^{−1} μ_k - \\frac{1}{2}log|Σ_k| + log π_k\\] QDA classifier involves plugging estimates for \\(Σ_k\\), \\(μ_k\\), and \\(π_k\\) into the above equation, and then assigning an observation X = x to the class for which this quantity is largest. The quantity x appears as a quadratic function, hence the name. Why the LDA to QDA is preferred or vice-versa? 1. Bias-variance trade-off - Pro LDA: LDA assumes that the K classes share a common covariance matrix and the quantity X becomes linear, which means there are \\(K_p\\) linear coefficients to estimate.LDA is a much less flexible classifier than QDA, and so has substantially lower variance; improved prediction performance. Con LDA: If the assumption K classes share a common covariance matrix is badly off, LDA can suffer from high bias Conclusion: Use LDA when there is a few training observations; use QDA when the training set is very large or common covariance matrix is untennable. Figure 4.8: Left: The Bayes (purple dashed), LDA (black dotted), and QDA (green solid) decision boundaries for a two-class problem with Σ1 = Σ2. The shading indicates the QDA decision rule. Since the Bayes decision boundary is linear, it is more accurately approximated by LDA than by QDA. Right: Details are as given in the left-hand panel, except that Σ1 ̸= Σ2. Since the Bayes decision boundary is non-linear, it is more accurately approximated by QDA than by LDA. 4.5.4 Naive Bayes Estimating a p-dimensional density function is challenging; naive bayes make a different assumption than LDA and QDA. an alternative to LDA that does not assume normally distributed predictors \\[f_k(x) = f_{k1}(x_1) × f_{k2}(x_2)×· · ·×f{k_p}(x_p),\\] where \\(f_{kj}\\) is the density function of the jth predictor among observations in the kth class Within the kth class, the p predictors are independent. Why naive Bayes is better/powerful? By assuming that the p covariates are independent within each class, we assumed that there is no association between the predictors! When estimating a p-dimensional density function, it is difficult to calculate the marginal distribution of each predictor and joint distribution of the predictors. Although p covariates might not be independent within each class, it is convenient and we obtain pretty decent results when the n is small, p is large. It reduces variance, though it has some bias (Bias-variance trade-off) Options to estimate the one-dimensional density function fkj using training data [For Quantitative \\(X_j\\)] -&gt; We assume \\(X_j |Y = k ∼ N(μ_{jk},σ_{jk}^2)\\), where within each class, the jth predictor is drawn from a (univariate) normal distribution. It is QDA-like with diagonal class-specific covariance matrix [For Quantitative \\(X_j\\)] -&gt; Use a non-parametric estimate for \\(f_{kj}\\). First, a histogram for the within-class observations and then estimate \\(f_{kj}(x_j)\\). Or else, use kernel density estimator. [For Qualitative \\(X_j\\)] -&gt;Count the proportion of training observations for the jth predictor corresponding to each class. Note: Fixing the threshold, the Naive Bayes has a higher error rate than LDA, but better prediction (higher sensitivity). "],["summary-of-the-classification-methods.html", "4.6 Summary of the classification methods", " 4.6 Summary of the classification methods 4.6.1 An Analytical Comparison LDA and logistic regression assume that the log odds of the posterior probabilities is linear in x. QDA assumes that the log odds of the posterior probabilities is quadratic in x. LDA is simply a restricted version of QDA with \\(Σ_1 = · · · = Σ_K = Σ\\) LDA is a special case of naive Bayes and vice-versa! LDA assumes that the features are normally distributed with a common within-class covariance matrix, and naive Bayes instead assumes independence of the features. Naive Bayes can produce a more flexible fit. QDA might be more accurate in settings where interactions among the predictors are important in discriminating between classes. LDA &gt; logistic regression when the observations at each Kth class is normal. K-nearest neighbors (KNN) will be better classifiers when decision boudary is non-linear, n is large, and p is small. KNN has low bias but large variance; as such, KNN requires a lot of observations relative to the number of predictors. If decision boundary is non-linear but n is and p are small, then QDA may be preferred to KNN. KNN does not tell us which predictors are important! Final note. The choice of method depends on (1) the true distribution of the predictors in each of the K classes,(2) the values of n and p - bias-variance trade-off 4.6.2 An Empirical Comparison Figure 4.9: Boxplots of the test error rates for each of the linear scenarios described in the main text. When Bayes decision boundary is linear, Scenario 1: Binary class response, equal observations in each class, uncorrelated predictors Scenario 2: Similar to Scenario 1, but the predictors had a correlation of −0.5. Scenario 3: Predictors had a negative correlation, t-distribution (more extreme points at the tails) Figure 4.10: Boxplots of the test error rates for each of the non-linear scenarios described in the main text When Bayes decision boundary is non-linear, Scenario 4: normal distiibution, correlation of 0.5 between the predictors in the first class, and correlation of −0.5 between the predictors in the second class. Scenario 5: Normal distribution, uncorrelated predictors Scenario 6: Normal distribution, different diagonal covariance matrix for each class, small n "],["generalized-linear-models.html", "4.7 Generalized Linear Models", " 4.7 Generalized Linear Models Count data (e.g. number of bikers per hour) is neither quantitative nor qualitative =&gt; neither linear regression nor the classification approaches considered so far are applicable. "],["linear-regression-with-count-data---negative-values.html", "4.8 Linear regression with count data - negative values", " 4.8 Linear regression with count data - negative values The results of fitting a least squares regression model to the Bikeshare data provides some reasonable results: as weather progressively worsens, the number of bikers decreases (coefficients become negative wrt baseline) the coefficients associated with season and time of day match expected patterns (lowest in winter, and highest during peak commute times) Figure 4.11: Results for a least squares linear model fit to predict bikers in the Bikeshare data. For the qualitative variable weathersit, the baseline level corresponds to clear skies. Figure 4.12: A least squares linear regression model was fit to predict bikers in the Bikeshare data set. Left: The coefficients associated with the month of the year. Bike usage is highest in the spring and fall, and lowest in the winter. Right: The coefficients associated with the hour of the day. Bike usage is highest during peak commute times, and lowest overnight. Problem 1: model predicts negative numbers of bikers at times "],["linear-regression-with-count-data---heteroscedasticity.html", "4.9 Linear regression with count data - heteroscedasticity", " 4.9 Linear regression with count data - heteroscedasticity In this example, the variance of biker numbers changes as the mean number changes: during worse conditions, there are few bikers, and little variation in the number of bikers during better conditions, there are many bikers on average, but also larger variation in the number of bikers Figure 4.13: Left: On the Bikeshare dataset, the number of bikers is displayed on the y-axis, and the hour of the day is displayed on the x-axis. For the most part, as the mean number of bikers increases, so does the variance in the number of bikers. A smoothing spline fit is shown in green. Right: The log of the number of bikers is displayed on the y-axis. Problem 2: observed heteroscedasticity is a violation of linear model assumptions \\[Y = \\beta_{0} + \\sum_{j=1}^p \\beta_{j} + \\epsilon\\] where \\(\\epsilon\\) is a mean-zero error term with a constant variance Transforming to log improves the variance, but cannot be used where the response can take on a 0 value. Log transformation also results in challenges in interpretation: e.g. “a one-unit increase in \\(X_j\\) is associated with an increase in the mean of the log of \\(Y\\) by an amount \\(β_j\\)” "],["problems-with-linear-regression-of-count-data.html", "4.10 Problems with linear regression of count data", " 4.10 Problems with linear regression of count data Problem 1: model predicts negative numbers of bikers at times Problem 2: observed heteroscedasticity is a violation of linear model assumptions Problem 3: integer values (bikers) predicted using a continuous response \\(Y\\) “[A] Poisson regression model provides a much more natural and elegant approach for this task.” "],["poisson-distribution.html", "4.11 Poisson distribution", " 4.11 Poisson distribution A count response variable \\(Y\\) (which takes on non-negative integer values) can be modeled using the Poisson distribution, where the probability that \\(Y\\) takes on a given count value \\(k\\) can be calculated as: \\(Pr(Y = k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}\\) for \\(k\\) = 0, 1, 2, … where \\(\\lambda\\) represents both the expected value (mean) and variance of \\(Y\\): \\(Y = E(Y) = Var(Y)\\) =&gt; “[I]f \\(Y\\) follows the Poisson distribution, then the larger the mean of \\(Y\\), the larger its variance.” par(mfrow = c(2,2)) lambda &lt;- c(1:4) k &lt;- c(0:10) for (lam in lambda) { Prk &lt;- (exp(-lam)*lam^k)/factorial(k) plot(k, Prk, type = &#39;b&#39;, ylim = c(0, 0.4), main = paste(&quot;lambda =&quot;, lam)) } Figure 4.14: Plots of Poisson Distributions with different lambda values, showing how variance increases with increasing lambda. Note all values are non-negative integer values, suitable for modelling counts, k. "],["poisson-regression-model-mean-lambda.html", "4.12 Poisson Regression Model mean (lambda)", " 4.12 Poisson Regression Model mean (lambda) “[R]ather than modeling [a count response variable], \\(Y\\), as a Poisson distribution with a fixed mean value like \\(\\lambda\\) = 5, we would like to allow the mean to vary as a function of the covariates.” The mean \\(\\lambda\\) can be modeled as a function of the predictor variables as follows: \\(log(\\lambda(X_1, ..., X_p) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p\\) NB: taking the log ensures that \\(\\lambda\\) can only be non-negative. This is equivalent to representing the mean \\(\\lambda\\) as follows: \\(\\lambda = \\text{E}(Y) = \\lambda(X_1, ..., X_p) = e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}\\) "],["estimating-the-poisson-regression-parameters.html", "4.13 Estimating the Poisson Regression parameters", " 4.13 Estimating the Poisson Regression parameters The calculation of \\(\\lambda\\) can then be used in the formula of the Poisson Distribution, allowing the Maximum Likelihood approach to be used in estimating the parameters, \\(\\beta_0\\), \\(\\beta_1\\),…, \\(\\beta_p\\): Poisson Distribution Formula: \\(Pr(Y = k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}\\) for \\(k\\) = 0, 1, 2, … Maximum likelihood: \\(l(\\beta_0, \\beta_1, ..., \\beta_p) = \\Pi_{i=1}^n\\frac{e^{-\\lambda(x_i)}\\lambda(x_i)^{y_i}}{y_i!}\\) where \\(\\lambda(x_i) = e^{\\beta_0 + \\beta_1x_{i1} + ... + \\beta_px_{ip}}\\) Coefficients that maximize the likelihood \\(l(\\beta_0, \\beta_1, ..., \\beta_p)\\) (make the observed data as likely as possible) are chosen. "],["interpreting-poisson-regression.html", "4.14 Interpreting Poisson Regression", " 4.14 Interpreting Poisson Regression An increase in \\(X_j\\) by one unit is associated with a change in \\(E(Y) = \\lambda\\) by a factor of \\(exp(\\beta_j)\\) Figure 4.15: Results for Poisson regression model fit to predict bikers in the Bikeshare data. For the qualitative variable weathersit, the baseline level corresponds to clear skies. A change in weather from clear to cloudy skies is associated with a change in mean bike usage by a factor of exp(-0.08) = 0.923 i.e. on average, only 92.3% as many people will use bikes compared to when it is clear (baseline weather). "],["advantages-of-poisson-regression.html", "4.15 Advantages of Poisson Regression", " 4.15 Advantages of Poisson Regression Poisson regression has several advantages in modeling count data: Mean-variance relationship We implicitly assume that mean bike usage in a given hour equals the variance of bike usage during that hour (cf use constant variance in linear regression). Non-negative fitted values There are no negative predictions using the Poisson regression model. "],["generalized-linear-models-1.html", "4.16 Generalized Linear Models", " 4.16 Generalized Linear Models Generalized linear models (GLMs) all follow the same ‘recipe’: use a set of predictors \\(X_1\\), …, \\(X_p\\) to predict a response \\(Y\\) model the response \\(Y\\) as coming from a particular distribution e.g. Poisson Distribution, for Poisson regression transform the mean of the response (via a link function \\(\\eta\\)) so that the transformed mean is a linear function of the predictors e.g. for Poisson regression, \\(log(\\lambda(X_1, ..., X_p) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p\\) "],["lab-classification-methods.html", "4.17 Lab: Classification Methods", " 4.17 Lab: Classification Methods "],["meeting-videos-2.html", "4.18 Meeting Videos", " 4.18 Meeting Videos 4.18.1 Cohort 1 Meeting chat log ADD LOG HERE "],["resampling-methods.html", "Chapter 5 Resampling Methods", " Chapter 5 Resampling Methods Learning objectives: Use a validation set to estimate the test error of a predictive model. Use leave-one-out cross-validation to estimate the test error of a predictive model. Use K-fold cross-validation to estimate the test error of a predictive model. Use the bootstrap to estimate the test error of a predictive model. Describe the advantages and disadvantages of the various methods for estimating model test error. "],["validation-set-approach.html", "5.1 Validation Set Approach", " 5.1 Validation Set Approach This involves randomly splitting the data into a training set and validation set. Note that in certain applications, such as time series analysis, it is not feasible to randomly split the data. The advantage of the validation set approach is that it is conceptually simple to understand and implement. However, the validation error rate is variable depending on the assignment of the training and validation sets. Additionally, we are giving up valuable data points by not using all of the data to estimate the model. Thus the validation error rate will tend to overestimate the test error rate. "],["validation-error-rate-varies-depending-on-data-set.html", "5.2 Validation Error Rate Varies Depending on Data Set", " 5.2 Validation Error Rate Varies Depending on Data Set Figure 5.1: Left: The validation set approach was used to estimated the test mean squared error from predicting mpg as a polynomial function of horsepower. Right: The estimated test error varies depending on the validation and training sets used. "],["leave-one-out-cross-validation-loocv.html", "5.3 Leave-One-Out Cross-Validation (LOOCV)", " 5.3 Leave-One-Out Cross-Validation (LOOCV) LOOCV aims to address some of the drawbacks of the validation set approach. Similar to validation set approach, LOOCV involves splitting the data into a training set and validation set. However, the validation set includes one observation, and the training set includes \\(n-1\\) observations. This process is repeated for all observations such that \\(n\\) models are estimated. Having a large training set avoids the problems from not using all (or almost all) of the data in estimating the model. Conversely, the validation error for a given model is highly variable since it consists of one observation, although it is unbiased. LOOCV estimate of test error is averaged over the \\(n\\) models: \\[CV_{n} = \\frac{1}{n}{\\sum_{i=1}^{n}}(y_{i}-\\hat{y_{i}})^2\\] "],["advantages-of-loocv-over-validation-set-approach.html", "5.4 Advantages of LOOCV over Validation Set Approach", " 5.4 Advantages of LOOCV over Validation Set Approach There are several advantages to LOOCV over validation set approach. It has less bias since models are repeatedly fitted on slightly different data sets, so it tends to not overestimate the test error as much as the validation set approach. The estimated test error will always be the same when LOOCV is performed on the entire data set. The major disadvantage to LOOCV is that it is computationally expensive. An easy shortcut for estimating the LOOCV test error for linear or polynomial regression models from a single model is as follows. \\[CV_{n} = \\frac{1}{n}{\\sum_{i=1}^{n}}\\left(\\frac{y_{i} - \\hat{y_{i}}}{1 - h_{i}}\\right)^2\\] where \\(h_{i}\\) is the leverage for a given residual as defined in equation 3.37 in the book for a simple linear regression. Its value falls between 1 and \\(1/n\\), so that observations whose residual has high leverage will contribute relatively more to the CV statistic. In general, LOOCV can be used for various kinds of models, including logistic regression, LDA, and QDA. "],["k-fold-cross-validation.html", "5.5 k-fold Cross-Validation", " 5.5 k-fold Cross-Validation This is an alternative to LOOCV which involves dividing the data set into \\(k\\) groups (folds) of approximately equal size. The percent of the data set that is in the validation set can be thought of as \\(1/k\\). E.g., for \\(k=5\\) groups, 20% of the data would be withheld for testing. "],["graphical-illustration-of-k-fold-approach.html", "5.6 Graphical Illustration of k-fold Approach", " 5.6 Graphical Illustration of k-fold Approach Figure 5.2: The data set is split five times such that all observations are included in one validation set. The model is estimated on 80% of the data five different times, the predictions are made for the remaining 20%, and the test MSEs are averaged. Thus, LOOCV is a special case of k-fold cross-validation, where \\(k=n\\). The equation for the CV statistic is below: \\[CV_{k} = \\frac{1}{k}{\\sum_{i=1}^{k}}MSE_{i}\\] "],["advantages-of-k-fold-cross-validation-over-loocv.html", "5.7 Advantages of k-fold Cross-Validation over LOOCV", " 5.7 Advantages of k-fold Cross-Validation over LOOCV The main advantage of k-fold over LOOCV is computational. However, there are other advantages related to the bias-variance tradeoff. The figure below shows the true test error for the simulated data sets from Chapter 2 compared to the LOOCV error and k-fold cross-validation error. Figure 5.3: The estimated test errors for LOOCV and k-fold cross validation is compared to the true test error for the three simulated data sets from Chapter 2. True test error is shown in blue, LOOCV error is the dashed black line, and 10-fold error is shown in orange. "],["bias-variance-tradeoff-and-k-fold-cross-validation.html", "5.8 Bias-Variance Tradeoff and k-fold Cross-Validation", " 5.8 Bias-Variance Tradeoff and k-fold Cross-Validation As mentioned previously, the validation approach tends to overestimate the true test error, but there is low variance in the estimate since we just have one estimate of the test error. Conversely, the LOOCV method has little bias, since almost all observations are used to create the models. Since the mean of the highly correlated \\(n\\) models has a higher variance, LOOCV mean estimated error has a higher variance. The k-fold method (\\(k&lt;n\\)) suffers from intermediate bias and variance levels. For this reason, \\(k=5\\) or \\(k=10\\) is often used in modeling since it has been demonstrated to yield results that do not have either too much bias or variance. "],["cross-validation-on-classification-problems.html", "5.9 Cross-Validation on Classification Problems", " 5.9 Cross-Validation on Classification Problems Previous examples have focused on measuring cross-validated test error where \\(Y\\) is quantitative. We can also use cross validation for classification problems, using the equation below, which is an example of LOOCV. \\(Err_{i} = I(Y_{i}\\neq\\hat{Y}_{i})\\) \\[CV_{n} = \\frac{1}{n}{\\sum_{i=1}^{n}}Err_{i}\\] "],["logistic-polynomial-regression-bayes-decision-boundaries-and-k-fold-cross-validation.html", "5.10 Logistic Polynomial Regression, Bayes Decision Boundaries, and k-fold Cross Validation", " 5.10 Logistic Polynomial Regression, Bayes Decision Boundaries, and k-fold Cross Validation Figure 5.4: Estimated decision boundaries of polynomial logistic regression models for simulated data are shown. The Bayes decision boundary is the dashed purple line. In practice, the true test error and Bayes error rate are unknown, so we need to estimate the test error rate. This can be done via k-fold cross-validation, which is often a good estimate of the true test error rate. Figure 5.5: The test error rate (beige), the training error rate (blue), and the 10-fold cross validation error rate (black) are shown for polynomial logistic regression and KNN classification. "],["the-bootstrap.html", "5.11 The Bootstrap", " 5.11 The Bootstrap The bootstrap can be used in a wide variety of modeling frameworks to estimate the uncertainty associated with a given estimator. For example, the bootstrap is useful to estimate the standard errors of a coefficient. The bootstrap involves repeated sampling with replacement from the original data set to form a distribution of the statistic in question. "],["population-distribution-compared-to-bootstrap-distribution.html", "5.12 Population Distribution Compared to Bootstrap Distribution", " 5.12 Population Distribution Compared to Bootstrap Distribution Figure 5.6: The distribution of the mean of alpha is shown on the left, with 1000 samples generated from the true population. A bootstrap distribution is shown in the middle, with 1000 samples taken from the original sample. Note that both confidence intervals contain the true alpha (pink line) in the right panel, and that the spread of both distributions is similar. "],["bootstrap-standard-error.html", "5.13 Bootstrap Standard Error", " 5.13 Bootstrap Standard Error The bootstrap standard error formula is somewhat more complex. The equation below gives the standard error for the \\(\\hat{\\alpha}\\) example in the book. The bootstrap standard error functions as an estimate of the standard error of \\(\\hat{\\alpha}\\) estimated from the original data set. \\[SE_{B(\\hat{\\alpha})} = \\sqrt{\\frac{1}{B-1}\\sum_{r=1}^{B}\\left(\\hat{\\alpha}^{*r} - \\frac{1}{B}\\sum_{j=1}^{B}\\hat{\\alpha}^{*j}\\right)^2}\\] "],["meeting-videos-3.html", "5.14 Meeting Videos", " 5.14 Meeting Videos 5.14.1 Cohort 1 Meeting chat log ADD LOG HERE "],["linear-model-selection-and-regularization.html", "Chapter 6 Linear Model Selection and Regularization", " Chapter 6 Linear Model Selection and Regularization Learning objectives: Select a subset of features to include in a linear model. Compare and contrast the forward stepwise, backward stepwise, hybrid, and best subset methods of subset selection. Use shrinkage methods to constrain the flexibility of linear models. Compare and contrast the lasso and ridge regression methods of shrinkage. Reduce the dimensionality of the data for a linear model. Compare and contrast the PCR and PLS methods of dimension reduction. Explain the challenges that may occur when fitting linear models to high-dimensional data. "],["slide-1.html", "6.1 Slide 1", " 6.1 Slide 1 Try to follow a slide-like format. "],["slide-2.html", "6.2 Slide 2", " 6.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-4.html", "6.3 Meeting Videos", " 6.3 Meeting Videos 6.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["moving-beyond-linearity.html", "Chapter 7 Moving Beyond Linearity", " Chapter 7 Moving Beyond Linearity Learning objectives: We’ll fill these in. "],["slide-1-1.html", "7.1 Slide 1", " 7.1 Slide 1 Try to follow a slide-like format. "],["slide-2-1.html", "7.2 Slide 2", " 7.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-5.html", "7.3 Meeting Videos", " 7.3 Meeting Videos 7.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["tree-based-methods.html", "Chapter 8 Tree-Based Methods", " Chapter 8 Tree-Based Methods Learning objectives: We’ll fill these in. "],["slide-1-2.html", "8.1 Slide 1", " 8.1 Slide 1 Try to follow a slide-like format. "],["slide-2-2.html", "8.2 Slide 2", " 8.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-6.html", "8.3 Meeting Videos", " 8.3 Meeting Videos 8.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["support-vector-machines.html", "Chapter 9 Support Vector Machines", " Chapter 9 Support Vector Machines Learning objectives: We’ll fill these in. "],["slide-1-3.html", "9.1 Slide 1", " 9.1 Slide 1 Try to follow a slide-like format. "],["slide-2-3.html", "9.2 Slide 2", " 9.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-7.html", "9.3 Meeting Videos", " 9.3 Meeting Videos 9.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["deep-learning.html", "Chapter 10 Deep Learning", " Chapter 10 Deep Learning Learning objectives: We’ll fill these in. "],["slide-1-4.html", "10.1 Slide 1", " 10.1 Slide 1 Try to follow a slide-like format. "],["slide-2-4.html", "10.2 Slide 2", " 10.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-8.html", "10.3 Meeting Videos", " 10.3 Meeting Videos 10.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["survival-analysis-and-censored-data.html", "Chapter 11 Survival Analysis and Censored Data", " Chapter 11 Survival Analysis and Censored Data Learning objectives: We’ll fill these in. "],["slide-1-5.html", "11.1 Slide 1", " 11.1 Slide 1 Try to follow a slide-like format. "],["slide-2-5.html", "11.2 Slide 2", " 11.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-9.html", "11.3 Meeting Videos", " 11.3 Meeting Videos 11.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["unsupervised-learning.html", "Chapter 12 Unsupervised Learning", " Chapter 12 Unsupervised Learning Learning objectives: We’ll fill these in. "],["slide-1-6.html", "12.1 Slide 1", " 12.1 Slide 1 Try to follow a slide-like format. "],["slide-2-6.html", "12.2 Slide 2", " 12.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-10.html", "12.3 Meeting Videos", " 12.3 Meeting Videos 12.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["multiple-testing.html", "Chapter 13 Multiple Testing", " Chapter 13 Multiple Testing Learning objectives: We’ll fill these in. "],["slide-1-7.html", "13.1 Slide 1", " 13.1 Slide 1 Try to follow a slide-like format. "],["slide-2-7.html", "13.2 Slide 2", " 13.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-11.html", "13.3 Meeting Videos", " 13.3 Meeting Videos 13.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["abbreviations.html", "Abbreviations", " Abbreviations Abbreviation Term RSE residual standard error RSS residual sum of squares TSS total sum of squares "],["latex.html", "Appendix: Bookdown and LaTeX Notes", " Appendix: Bookdown and LaTeX Notes Ray’s Formatting Notes: version 2021-09-22 https://gist.github.com/RaymondBalise/ee4b7da0a70087317dc52bf479a4e2b6 .col2 { columns: 2 150px; /* number of columns and width in pixels*/ -webkit-columns: 2 150px; /* chrome, safari */ -moz-columns: 2 150px; /* firefox */ } .col3 { columns: 3 100px; -webkit-columns: 3 100px; -moz-columns: 3 100px; } "],["markdown-highlighting.html", "Markdown highlighting", " Markdown highlighting Formatting Code bold **bold** bold __bold__ italic *italic* italic _italic_ "],["text-coloring.html", "Text coloring", " Text coloring To add color you can use CSS or R code like this: color_text &lt;- function(x, color){ if(knitr::is_latex_output()) paste(&quot;\\\\textcolor{&quot;,color,&quot;}{&quot;,x,&quot;}&quot;,sep=&quot;&quot;) else if(knitr::is_html_output()) paste(&quot;&lt;font color=&#39;&quot;,color,&quot;&#39;&gt;&quot;,x,&quot;&lt;/font&gt;&quot;,sep=&quot;&quot;) else x } red &lt;- function(x){ if(knitr::is_latex_output()) paste(&quot;\\\\textcolor{&quot;,&#39;red&#39;,&quot;}{&quot;,x,&quot;}&quot;,sep=&quot;&quot;) else if(knitr::is_html_output()) paste(&quot;&lt;font color=&#39;red&#39;&gt;&quot;,x,&quot;&lt;/font&gt;&quot;,sep=&quot;&quot;) else x } The strng This is colored with the red function comes from: `r red(&quot;This is colored with the red function&quot;)` The string This is colored with the color_text function set to mauve comes from: `r color_text(&quot;This is colored with the color_text function set to mauve&quot;, &quot;#E0B0FF&quot;)` "],["x99-4.html", "Section references", " Section references Section 99.4 comes from Section [99.4](#x99-4) "],["footnotes.html", "Footnotes", " Footnotes 1 comes from ^[A footnote] A footnote↩︎ "],["formatting-text.html", "Formatting Text", " Formatting Text Appearance Code Three xfun::n2w(3, cap = TRUE) 14.5% scales::percent(0.1447, accuracy= .1) p=0.145 scales::pvalue(0.1447, accuracy= .001, add_p =TRUE) "],["figures.html", "Figures", " Figures Name a figure chunk with a name like figure99-1 and include fig.cap=“something” then reference it like this: \\@ref(fig:figure99-1) "],["displaying-formula.html", "Displaying Formula", " Displaying Formula Formatting To tweak the appearance of words use these formats: Formatting Code Looks like plain text \\text{text Pr} \\(\\text{text Pr}\\) bold Greek symbol \\boldsymbol{\\epsilon} \\(\\boldsymbol{\\epsilon}\\) typewriter \\tt{blah} \\(\\tt{blah}\\) slide font \\sf{blah} \\(\\sf{blah}\\) bold \\mathbf{x} \\(\\mathbf{x}\\) plain \\mathrm{text Pr} \\(\\mathrm{text Pr}\\) cursive \\mathcal{S} \\(\\mathcal{S}\\) Blackboard bold \\mathbb{R} \\(\\mathbb{R}\\) Symbols Symbols Code \\(\\stackrel{\\text{def}}{=}\\) \\stackrel{\\text{def}}{=} Notation Based on: https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html Math Code \\(x = y\\) $x = y$ \\(x \\approx y\\) $x \\approx y$ \\(x &lt; y\\) $x &lt; y$ \\(x &gt; y\\) $x &gt; y$ \\(x \\le y\\) $x \\le y$ \\(x \\ge y\\) $x \\ge y$ \\(x \\ge y\\) $x \\ge y$ \\(x \\times y\\) $x \\times y$ \\(x^{n}\\) $x^{n}$ \\(x_{n}\\) $x_{n}$ \\(\\overline{x}\\) $\\overline{x}$ \\(\\hat{x}\\) $\\hat{x}$ \\(\\widehat{SE}\\) $\\widehat{SE}$ \\(\\tilde{x}\\) $\\tilde{x}$ \\(\\frac{a}{b}\\) $\\frac{a}{b}$ \\(\\displaystyle \\frac{a}{b}\\) $\\displaystyle \\frac{a}{b}$ \\(\\binom{n}{k}\\) $\\binom{n}{k}$ \\(x_{1} + x_{2} + \\cdots + x_{n}\\) $x_{1} + x_{2} + \\cdots + x_{n}$ \\(x_{1}, x_{2}, \\dots, x_{n}\\) $x_{1}, x_{2}, \\dots, x_{n}$ \\(\\mathbf{x} = \\langle x_{1}, x_{2}, \\dots, x_{n}\\rangle\\) $\\mathbf{x} = \\langle x_{1}, x_{2}, \\dots, x_{n}\\rangle$ \\(x \\in A\\) $x \\in A$ \\(|A|\\) $|A|$ \\(x \\in A\\) $x \\in A$ \\(x \\subset B\\) $x \\subset B$ \\(x \\subseteq B\\) $x \\subseteq B$ \\(A \\cup B\\) $A \\cup B$ \\(A \\cap B\\) $A \\cap B$ \\(X \\sim {\\sf Binom}(n, \\pi)\\) X \\sim {\\sf Binom}(n, \\pi)$ \\(\\mathrm{P}(X \\le x) = {\\tt pbinom}(x, n, \\pi)\\) $\\mathrm{P}(X \\le x) = {\\tt pbinom}(x, n, \\pi)$ \\(P(A \\mid B)\\) $P(A \\mid B)$ \\(\\mathrm{P}(A \\mid B)\\) $\\mathrm{P}(A \\mid B)$ \\(\\{1, 2, 3\\}\\) $\\{1, 2, 3\\}$ \\(\\sin(x)\\) $\\sin(x)$ \\(\\log(x)\\) $\\log(x)$ \\(\\int_{a}^{b}\\) $\\int_{a}^{b}$ \\(\\left(\\int_{a}^{b} f(x) \\; dx\\right)\\) $\\left(\\int_{a}^{b} f(x) \\; dx\\right)$ \\(\\left[\\int_{-\\infty}^{\\infty} f(x) \\; dx\\right]\\) $\\left[\\int_{\\-infty}^{\\infty} f(x) \\; dx\\right]$ \\(\\left. F(x) \\right|_{a}^{b}\\) $\\left. F(x) \\right|_{a}^{b}$ \\(\\sum_{x = a}^{b} f(x)\\) $\\sum_{x = a}^{b} f(x)$ \\(\\prod_{x = a}^{b} f(x)\\) $\\prod_{x = a}^{b} f(x)$ \\(\\lim_{x \\to \\infty} f(x)\\) $\\lim_{x \\to \\infty} f(x)$ \\(\\displaystyle \\lim_{x \\to \\infty} f(x)\\) $\\displaystyle \\lim_{x \\to \\infty} f(x)$ ` \\(RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (Y_n - \\hat{Y}_i)^2}\\) $RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (Y_n - \\hat{Y}_i)^2}$ ` "],["equations.html", "Equations", " Equations These are formulas that appear with an equation number. Basic Equation The names of equations can not include . or _ but it can include - \\begin{equation} 1 + 1 = 2 (\\#eq:eq99-1) \\end{equation} Which appears as: \\[\\begin{equation} 1 + 1 = 2 \\tag{13.1} \\end{equation}\\] The reference to the equation is (13.1) which comes from this code \\@ref(eq:eq99-1) Case-When Equation (Large Curly Brace) Based on: https://tex.stackexchange.com/questions/9065/large-braces-for-specifying-values-of-variables-by-condition Case when formula: \\[\\begin{equation} y = \\begin{cases} 0, &amp; \\text{if}\\ a=1 \\\\ 1, &amp; \\text{otherwise} \\end{cases} \\tag{13.2} \\end{equation} \\] Which comes from this code: \\begin{equation} y = \\begin{cases} 0, &amp; \\text{if}\\ a=1 \\\\ 1, &amp; \\text{otherwise} \\end{cases} (\\#eq:eq99-2) \\begin{equation} The reference to equation is (13.2) which comes from this code \\@ref(eq:eq99-2) Alligned with Underbars \\[\\begin{equation} \\begin{aligned} \\mathrm{E}(Y-\\hat{Y})^2 &amp; = \\mathrm{E}[f(X) + \\epsilon -\\hat{f}(X)]^2 \\\\ &amp; = \\underbrace{[f(X) -\\hat{f}(X)]^2}_{\\mathrm{Reducible}} + \\underbrace{\\mathrm{var}(\\epsilon)}_{\\mathrm{Irreducible}} \\\\ \\end{aligned} \\tag{13.3} \\end{equation}\\] Comes from this code: \\begin{equation} \\begin{aligned} \\mathrm{E}(Y-\\hat{Y})^2 &amp; = \\mathrm{E}[f(X) + \\epsilon -\\hat{f}(X)]^2 \\\\ &amp; = \\underbrace{[f(X) -\\hat{f}(X)]^2}_{\\mathrm{Reducible}} + \\underbrace{\\mathrm{var}(\\epsilon)}_{\\mathrm{Irreducible}} \\\\ \\end{aligned} (\\#eq:eq99-3) \\end{equation} "],["greek-letters.html", "Greek letters", " Greek letters Based on: https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html letters code \\(\\alpha A\\) $\\alpha A$ \\(\\beta B\\) $\\beta B$ \\(\\gamma \\Gamma\\) $\\gamma \\Gamma$ \\(\\delta \\Delta\\) $\\delta \\Delta$ \\(\\epsilon \\varepsilon E\\) $\\epsilon \\varepsilon E$ \\(\\zeta Z \\sigma\\) $\\zeta Z \\sigma \\(\\eta H\\) $\\eta H$ \\(\\theta \\vartheta \\Theta\\) $\\theta \\vartheta \\Theta$ \\(\\iota I\\) $\\iota I$ \\(\\kappa K\\) $\\kappa K$ \\(\\lambda \\Lambda\\) $\\lambda \\Lambda$ \\(\\mu M\\) $\\mu M$ \\(\\nu N\\) $\\nu N$ \\(\\xi\\Xi\\) $\\xi\\Xi$ \\(o O\\) $o O$ (omicron) \\(\\pi \\Pi\\) $\\pi \\Pi$ \\(\\rho\\varrho P\\) $\\rho\\varrho P$ \\(\\sigma \\Sigma\\) \\sigma \\Sigma$ \\(\\tau T\\) $\\tau T$ \\(\\upsilon \\Upsilon\\) $\\upsilon \\Upsilon$ \\(\\phi \\varphi \\Phi\\) $\\phi \\varphi \\Phi$ \\(\\chi X\\) $\\chi X$ \\(\\psi \\Psi\\) $\\psi \\Psi$ \\(\\omega \\Omega\\) $\\omega \\Omega$ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
